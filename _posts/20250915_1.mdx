---
title: 모든 예측은 선형과 비선형 사이에 있다
description: 우리는 종종 선형과 비선형 사이의 스펙트럼에서 예측을 합니다.
thumbnail: https://imagedelivery.net/6qzLODAqs2g1LZbVYqtuQw/9538f114-52c8-414b-4b46-157e64bf0a00/public
prerequisites:
  ['우원', 'FE Developer', 'FE Developer', '안녕하세요! 우원입니다.']
stacks: ['MATH', 'PHILOSOPHY', 'STOCHASTIC']
writer: 우원
date: '2025-09-15'
name: '20250915_1'
lock: 'false'
---

## 모든 예측은 선형과 비선형 사이에 있다

“세상의 모든 움직임은 수로 환원될 수 있으며, 모든 수의 관계는 직선으로 설명될 수 있다.”
18세기, 어느 계몽주의 수학자의 믿음

21세기 인공지능 시대의 예측은 과거 어느 때보다 정교해졌습니다.
주가의 흐름, 소비자의 행동, 기후의 변화, 심지어 언어의 생성까지도 수치화된 벡터 공간 위에서 모형화되고 예측됩니다.
하지만 놀랍게도, 이 복잡한 예측의 대부분은 단 하나의 질문에서 시작됩니다.

“이 관계는 선형(linear)인가, 비선형(nonlinear)인가?”

### 1. 선형이라는 단순함

선형 모델은 아름답습니다(실제로도 그렇습니다!)  
예측값은 독립 변수들의 단순 가중합으로 표현되고,  
이 선형식은 대수적으로 직관적일 뿐 아니라  
미분 가능한 특성 덕분에 최적화 또한 쉬운 구조를 가집니다.  
실제로 수많은 통계 기법의 근간에는 선형회귀(linear regression)가 놓여 있죠.

그 이유는 단순합니다.  
바로, 선형성은 다음 네 가지 강력한 가정을 가능하게 하기 때문입니다.

1. 선형독립(linear independence): 변수 간에 발생하는 다중공선성 회피, 해석 가능성 확보.
2. 정규분포(normal distribution): 잔차의 정규분포.
3. 선형결합(linear combination): 변수 간 효과를 선형적으로 추론.
4. 예측가능성(prediction under convexity): 오차 함수가 볼록(convex)하므로 전역 최적화를 보장.

하지만 우리가 사는 현실 세계는 놀랍도록 복잡합니다.  
특히 경제, 기후, 생물학, 금융 데이터는  
비선형성과 상호작용의 혼재 속에서 구조적으로 왜곡되어 있죠.  
그렇기에 선형의 세계는 단순화된 근사로 볼 수 있고,  
이 근사는 늘 특정 조건 하에서만 유효하다고 할 수 있습니다.

### 2. 왜 비선형이 중요한가요?

비선형성을 이해하는 일은 곧 현실의 상호작용을 이해하는 일입니다.  
가령, 금리와 주가 사이의 관계는 일정 구간에서는 선형적으로 보일 수 있습니다.  
하지만 특정 임계점(threshold)을 넘어서면 관계는 급격히 꺾이고,  
예측 불가능한 전이(transformation) 상태로 접어들게 되죠.  
이 현상은 수학적으로 비선형 함수의 위상적 변화로 설명될 수 있습니다.

또한, 다음과 같은 문제들은 선형 모델로는 근본적으로 다룰 수 없습니다.

- XOR 문제: 가장 간단한 논리 문제조차 선형 분리 불가능.  
  (컴퓨터공학에서 인공지능의 기초를 배우셨으면 마빈 민스키의 XOR 문제를 기억하실겁니다!)
- 커널 방법의 필요성: Support Vector Machine이 커널을 도입하는 이유는 고차원에서의 선형성 확보를 위한 것.
- 신경망의 활성화 함수: ReLU, Sigmoid, Tanh 등은 모두 비선형성 없이는 작동하지 않음.
  선형 함수만 쌓는다면 아무리 많은 층을 구성해도 단일 선형 변환에 불과.

정리하면 비선형성은 곧 비가역성, 민감성, 혼돈, 위상변화, 피드백 루프를 포함하고,  
이는 곧 현실 그 자체를 의미한다는 것입니다.

제2장. 선형 회귀의 철학과 한계

> “모든 것이 직선으로 설명될 수 있다면, 세계는 예측 가능하다.”

### 1. 선형 회귀는 왜 강력한가?

선형 회귀(linear regression)는 통계학에서 가장 오래되고, 가장 널리 사용되는 예측 기법입니다.  
두 변수 간의 관계를 분석하는 가장 간단한 형태인 단순 선형 회귀는 다음과 같은 식으로 표현됩니다.

y = $\beta_0 + \beta_1 x + \epsilon$

여기서

$\beta_0$

는 절편(intercept),

$\beta_1$

는 기울기(slope),

$\epsilon$은

오차항(error term)입니다.

이 간단한 식이 수많은 실제 문제에 적용 가능한 이유는 다음과 같은 철학적·수학적 배경에 있죠.

- 오컴의 면도날(Occam’s Razor): 간결한 모델이 선호되는 것. 선형 모델은 가장 단순한 함수 구조.
- 최소제곱법(Least Squares): 모델의 파라미터는 오차 제곱합을 최소화하는 방식으로 도출,
  이는 폐쇄형 해(closed-form solution)가 존재하는 드문 경우.
- 편의성과 해석력: 변수의 계수(beta)를 통해 각 독립변수의 영향력을 직관적으로 해석 가능.
- 확률적 해석: 잔차(e)가 정규분포를 따른다는 가정 하에, 회귀 계수의 신뢰구간, 유의확률(p-value), 결정계수(R²) 등 다양한 검정과 설명력이 가능.

(폐쇄형 해는 문제의 해가 유한한 개수의 표준적인 함수와 연산(덧셈, 곱셈, 나눗셈, 거듭제곱, 삼각함수, 로그, 지수 등)을 사용해 직접적으로 표현되는 해를 말합니다.
우리가 흔히 2차 방정식의 해를 구할 때 사용하는 근의 공식이 폐쇄형 해의 독보적 예입니다!)

하지만 이 모든 강점은 몇 가지 가정들 위에서만 성립합니다.
가정은 다음과 같습니다.

1. 선형성(linearity): 종속변수는 독립변수들의 선형결합으로 설명.
2. 독립성(independence): 오차항은 서로 독립.
3. 등분산성(homoscedasticity): 모든 관측값에서 오차의 분산이 동일.
4. 정규성(normality): 오차항은 정규분포를 따름.
5. 다중공선성 없음(no multicollinearity): 독립변수 간 상관관계가 낮음.

이 중 단 하나라도 깨지면, 회귀 모형의 신뢰성과 예측력은 급격히 떨어지게 됩니다.

(ADP나 빅분기를 공부하신 분들은 이 가정들을 잘 알고 계실 겁니다~)

### 2. 현실은 직선이 아니다

선형 회귀의 가정은 이상적인 수학 세계에서는 유효하지만, 실제 세계는 그렇지 않습니다.

#### 사례 1: 구조적 전이 (Structural Break)

- 예: 부동산 가격이 일정 수준까지는 소득에 비례하여 상승하지만,
  어느 임계점(threshold)을 넘어서면 급격한 투기적 거품 형성으로 선형성이 붕괴됩니다.
- 이러한 경우, 선형회귀는 평균의 착시를 만들어내며, 실제로는 존재하지 않는 관계를 상정하게 만듭니다.

#### 사례 2: 상호작용 효과 (Interaction Effects)

- 예: 광고비와 브랜드 인지도는 각각 매출에 영향을 줄 수 있으나,
  이 둘이 결합되어 더 큰 영향을 주는 경우, 단순 선형 회귀로는 설명할 수 없습니다.
- 이를 설명하기 위해서는 교호항(interaction term)을 추가해야 하며, 이는 점점 비선형 모형의 확장으로 수렴하게 됩니다.

#### 사례 3: 잔차의 비정규성과 이상치

- 현실 데이터는 종종 두꺼운 꼬리 분포(fat-tailed distribution), 이상치(outlier), 비정상성(non-stationarity)을 포함합니다.
- 이 경우, 회귀계수의 유의성 자체가 무의미하게 되며, 예측보다는 오차가 더 신뢰됩니다.

### 3. 선형을 넘어서기 위한 노력들

선형 회귀의 한계는 다양한 방식으로 보완되어 왔습니다

- 다항 회귀(Polynomial Regression): 선형 모델이지만 입력 변수의 차수를 높임으로써 비선형성을 표현
- 정규화 회귀(Ridge, Lasso): 과적합(overfitting)을 방지하기 위한 패널티 기반 회귀.
- 일반화 선형 모형(GLM): 분포와 연결함수를 자유롭게 선택하여 선형 회귀의 범용성 확장.
- 비선형 회귀(NLR): 비선형 함수를 직접 가정하여 파라미터 추정.
- 신경망(Neural Networks): 비선형성의 극단, 완전한 함수 근사기.

이러한 노력은 모두 현실의 복잡성을 설명하고자 하는 시도로  
선형성이라는 고전적 프레임의 틀에서 벗어나려는 운동이었습니다!

## 제3장. 비선형 회귀와 신경망: 고차원 함수 근사의 가능성

“세계는 선형이 아니다. 따라서 선형의 언어로 세계를 말할 수 없다.”

### 1. 비선형 회귀란 무엇인가?

비선형 회귀(Nonlinear Regression)는 독립 변수와 종속 변수 간의  
관계를 비선형 함수로 가정하는 예측 방법입니다.

일반적인 형태는 다음과 같죠:

$$
y = f(x; \theta) + \epsilon
$$

여기서

$f$

는 비선형 함수이고,
파라미터

$\theta$

는 폐쇄형 해(closed-form solution)를 가지지 않는 경우가 대부분입니다.  
따라서 수치 최적화 기법(Nelder-Mead, BFGS, SGD 등)을 이용한 추정이 필요하죠.

비선형 회귀는 단순히 형태만 복잡한 것이 아니라,  
근본적으로 모델의 표현력(expressive power)을 변화시킵니다.  
선형 회귀가 고정된 차원의 평면만을 다룬다면,  
비선형 회귀는 곡률(curvature), 경계값, 비대칭성, 상호작용의 변이 등을 포착할 수 있습니다.

### 2. 신경망: 비선형 회귀의 결정적 구조

신경망(Neural Network)은 가장 유연하고 강력한 비선형 회귀 모델입니다.  
그 구조는 수학적으로 다음과 같이 정의됩니다:

#### 다층 퍼셉트론(MLP):

$$
\begin{aligned}
\text{Input Layer: } & x \in \mathbb{R}^d \\
\text{Hidden Layer 1: } & h_1 = \sigma(W_1 x + b_1) \\
\text{Hidden Layer 2: } & h_2 = \sigma(W_2 h_1 + b_2) \\
\vdots \\
\text{Output Layer: } & \hat{y} = W_n h_{n-1} + b_n
\end{aligned}
$$

여기서

$W_i$

는 가중치 행렬,

$b_i$

는 편향 벡터,

$\sigma$

는 비선형 활성화 함수입니다(e.g., ReLU, Sigmoid, Tanh)

이 구조의 핵심은 단 하나입니다!

비선형 함수

$\sigma$

를 층층이 조합함으로써 복잡한 함수를 근사할 수 있다는 것입니다.
(Universal Approximation Theorem에서 더 자세히 다루겠습니다!)

### 3. Universal Approximation Theorem (보편근사정리)

> “적당한 조건 하에서, 단 하나의 은닉층을 가진 신경망도 모든 연속함수를 임의의 오차 이하로 근사할 수 있다.”

(정의를 보아도 이해가 잘 되지 않을 수 있습니다.  
그렇지만, 너무 자세히 설명하면 오히려 복잡해질 수 있기 때문에 빠르게 넘어가겠습니다!)

이 정리는 1989년 Cybenko와 Hornik 등의 연구를 통해 수학적으로 증명되었습니다.  
그 내용은 다음과 같습니다:

$
\sigma
$

가 비선형이며 바운드된 연속 함수이면,  
임의의 연속 함수

$f: \mathbb{R}^n \to \mathbb{R}$

와 임의의 오차

$\varepsilon > 0$

에 대해, 적당한 개수의 노드를 가진 은닉층 하나로 구성된 신경망

$\hat{f}$

가 존재하여,

$$
\left| f(x) - \hat{f}(x) \right| < \varepsilon \quad \forall x \in \text{compact domain}
$$

즉, 신경망은 이론적으로 모든 함수를 근사할 수 있는 구조를 가지고 있습니다.  
이는 기존의 선형 회귀가 본질적으로 다룰 수 없던 패턴 비선형 경계, 분기점, 고차 상호작용 등을  
신경망이 다룰 수 있다는 결정적 수학적 근거가 됩니다.

### 4. 현실 데이터에서의 적용: 비선형 예측

#### 사례 1. 의료: 폐 기능 예측

선형 회귀로는 폐활량과 나이의 선형 관계만을 잡을 수 있으나,  
신경망은 특정 연령 이후 급격히 감소하는 패턴을 포착할 수 있습니다.  
→ 비선형적인 건강 전이(transitional nonlinearity)

#### 사례 2. 금융: 옵션 가격

Black-Scholes 공식은 선형적인 가격 결정을 하지 않고,  
내재 변동성, 무위험 이자율, 행사가 등 다양한 요인이 비선형적으로 작용해 결정하게 됩니다.  
→ 최근 연구는 신경망 기반 옵션 가격 예측 모형을 이용해 기존 공식을 대체하려는 시도를 하고 있습니다.

#### 사례 3. 자연어 처리: 의미의 복잡성

“나는 너를 사랑하지 않는다” vs. “나는 사랑하지 않는다 너를”  
선형 회귀는 단어의 위치 정보나 어순을 포착할 수 없습니다.  
그에 비해 신경망은 이산 구조(단어)를 연속 벡터로 임베딩하고,  
다층 구조를 통해 의미의 뉘앙스를 포착할 수 있습니다.  
→ Transformer 등의 모델은 이 구조 위에 구축되어 있습니다.

### 5. 하지만, 비선형은 만능이 아니다

비선형 회귀와 신경망은 강력하지만, 다음과 같은 문제를 항상 안고 있습니다:

- 과적합(Overfitting): 훈련 데이터에만 지나치게 적합되어 일반화 실패.
- 해석 불가능성: 선형 모델처럼 계수 하나하나의 의미 해석이 불가.
- 최적화의 어려움: 수렴 불가, 로컬 미니마 등.
- 설명력보다 성능 위주: 특히 과학적 예측보다 산업적 예측에 적합.

## 제4장. 혼돈과 민감성: 비선형 동역학에서의 예측 불가능성

> “모든 것을 알면 모든 것을 예측할 수 있을까?”  
> — 라플라스의 악마, 1814

우리는 보통, 예측이란 데이터의 양이 충분하고, 모델이 정교하며,  
계산 능력이 충분하다면 정확히 할 수 있는 것이라 믿습니다.  
그러나 자연에는, 심지어 완전히 결정론적인 시스템조차도  
장기적으로는 예측이 불가능한 구조를 가진 경우가 존재합니다.

이것이 바로 혼돈(Chaos)이며, 예측 이론에서 비선형성보다 더 근본적인 한계로 작용합니다.

### 1. 결정론적 혼돈이란 무엇인가?

결정론적 혼돈(deterministic chaos)은 무작위성(randomness) 없이도,  
극단적으로 예측이 어려운 행동을 보이는 시스템을 말합니다.

이는 다음의 세 가지 조건을 만족하는 비선형 동역학계에서 발생합니다:

1. 민감한 초기 조건(Sensitive dependence on initial conditions)
2. 점근적 불안정성(Topological Mixing)
3. 조밀한 주기궤도(Dense Periodic Orbits)

즉, 시스템의 모든 정보가 주어진다 하더라도,
아주 작은 초기 오차가 시간이 지남에 따라 기하급수적으로 커지면서 예측을 무너뜨린다는 것입니다.

### 2. 나비효과: 민감한 초기 조건의 상징

> “브라질에서 나비가 날갯짓을 하면, 텍사스에 폭풍이 분다.”  
> — 에드워드 로렌츠, 1963

로렌츠(Lorenz)는 대기 예측 모델을 만들기 위해 단순한 3차 비선형 미분방정식 시스템을 만들었고,  
이를 시뮬레이션하던 중, 소수점 아래 3자리만 입력한 초기값이 예측 결과를 전혀 다르게 만든다는 사실을 발견했습니다.

$$
\begin{cases}
\frac{dx}{dt} = \sigma(y - x) \\
\frac{dy}{dt} = x(\rho - z) - y \\
\frac{dz}{dt} = xy - \beta z
\end{cases}
$$

이 로렌츠 방정식(Lorenz System)은 현대 혼돈 이론의 시작점이자,  
비선형 동역학계의 대표적인 예로,

특정 파라미터

$\sigma = 10, \rho = 28, \beta = 8/3$

하에서 극도로 민감한 비선형 궤적을 만들어냅니다.  
즉, 이 방정식은 완전히 결정적인 수학식임에도 불구하고,  
예측은 장기적으로 무용지물이 된다는 것을 보여줍니다.

### 3. 예측 불가능성은 예외가 아니라 구조다

#### 사례 1. 금융 시장의 비선형성

주가나 환율 등의 시간적 흐름은 대부분 비선형 시계열 데이터로 모델링됩니다.  
ARCH, GARCH 모델조차 조건부 이분산성(heteroskedasticity)을 포함하고 있음에도,  
극단적 사건(블랙스완)을 설명하지 못합니다.  
(이는 근본적으로 금융 시스템이 자기회귀적이며 비선형적인 상호작용 구조를 갖기 때문입니다!)

#### 사례 2. 생태계와 인구 모델

로지스틱 맵 (Logistic Map):

$$
x_{t+1} = r x_t (1 - x_t)
$$

$r \in [0, 4]$

에서 변화시, 일정 구간부터 혼돈적 진동 발생하는데

$r = 3.57$

이상부터는 완전한 혼돈으로 진입하고 예측 불가능한 패턴을 나타내게 됩니다.  
(이는 간단한 2차 함수 하나가 만들어내는 혼돈을 보여주는 대표적인 예입니다!)

#### 사례 3. 뇌 신호와 신경 다이내믹스

EEG 패턴, 뉴런 발화 모델 등은 혼돈적 특성을 보입니다.
특정 자극이 뇌 전체의 상태에 따라 전혀 다른 결과를 낳죠.
인지 예측 및 반응 예측의 어려움은 본질적으로 비선형이고 비가역적인 뇌 구조 때문입니다.

### 4. 시계열 데이터의 함정과 비선형 모델의 오용

많은 머신러닝 모델이 시계열 예측에 사용되지만, 다음의 함정이 존재합니다.

단기 예측은 가능하나, 장기 예측은 불가능하다는 것!  
: 예측 정확도는 시간 축이 늘어날수록 지수적으로 떨어지기 때문입니다.

지나치게 정합한 비선형 모델은 오히려 혼돈을 야기한다는 것!
: 실제로는 혼돈적인 구조인데, 신경망이 이를 단순한 주기 또는 경향성으로 ‘오해’하는 경우가 있습니다.

과잉 신뢰의 오류
: Deep Learning이 장기 예측을 잘한다는 오해는, 시스템의 비가역적 특성을 간과한 것입니다.

### 5. 예측의 한계, 그리고 우리가 할 수 있는 것

그렇다면 예측은 의미 없는가? 아닙니다.  
예측은 단기적으로는 여전히 유효한데,  
예측 자체보다도 “민감도”, “경계점”, “시스템의 안정성”을 파악하는 것이 더욱 중요합니다.

- Lyapunov 지수:
  민감도의 척도. 이 값이 양수면 혼돈, 음수면 안정.

- 위상 공간 분석:
  데이터의 궤적을 고차원에서 복원하여, 시스템의 동역학적 성질을 이해.

- 예측보다 제어(Control):
  예측이 어렵다면, 시스템을 제어하거나 안정된 구간으로 유지하려는 접근이 필요.

## 제5장. 선형과 비선형의 경계 – 하이브리드 모델의 필요성과 미래

> “선형은 설명하고, 비선형은 잡아낸다.”  
> — 예측 모델의 두 얼굴

선형 회귀는 예측의 시작이었고, 신경망은 예측의 확장이었습니다.  
하지만 우리가 실전에서 마주하는 데이터는 때로는 선형적이며, 때로는 비선형적입니다.
이러한 혼합적 구조를 다루기 위해 등장한 것이 바로 하이브리드 모델(Hybrid Model)입니다.

### 1. 선형의 해석력, 비선형의 표현력

#### 선형 모델의 장점

- 계수의 해석 가능성: 각 독립변수가 결과에 미치는 영향을 정량적으로 파악 가능
- 검정, 신뢰구간, 가설 설정 등 통계적 기반에서 유리
- 빠르고 안정적인 학습 가능

#### 비선형 모델의 장점

- 복잡한 함수 근사 가능: 상호작용, 계단 함수, 경계 조건 등 포착
- 고차원, 고비선형성 문제에 효과적
- 데이터 기반 최적 구조 탐색 가능

그러나 설명력 vs. 예측력의 딜레마는 여전히 존재합니다.  
→ 이를 해소하려는 노력이 모형의 결합, 즉 하이브리드 모델로 이어집니다.

### 2. 대표적 하이브리드 모델 구조

#### (1) GAM: Generalized Additive Model

$$
y = \beta_0 + f_1(x_1) + f_2(x_2) + \cdots + f_k(x_k) + \epsilon
$$

- 각 변수별로 비선형 함수 $f_i$를 적용
- 전체 구조는 선형 결합이지만, 개별 변수의 영향은 자유롭게 비선형 표현
- 설명력 + 유연성을 동시에 확보

예: 의료 데이터에서 연령은 곡선 관계, 성별은 범주형, 나머지는 선형 → GAM이 적합

#### (2) ARIMA + Neural Network

$$
\text{예측값} = \text{ARIMA 예측} + \text{NN 보정값}
$$

- 시계열 데이터의 선형 추세는 ARIMA로 처리
- 비선형 노이즈나 패턴은 NN으로 보정
- 예측력을 개선하면서 해석력을 일부 유지

예: 주가 예측에서 ARIMA가 시계열의 구조를 잡고, NN이 급격한 급등락을 보정

### 3. 실제 적용 사례

#### ① 금융

- 신용평가 → 선형 로지스틱 회귀 + 비선형 NN (핀테크 기업들이 점수화 모델에 활용)
- 채권 가격 예측 → 금리 추세는 선형, 스프레드 이상치는 NN

#### ② 마케팅

- 고객 이탈 예측 → 선형 변수(가입일, 연령) + NN으로 감정/이벤트 패턴 보정
- 추천 시스템 → 과거 클릭 패턴은 선형 모델, 관심사 탐색은 딥러닝

#### ③ 의료 진단

- GAM을 사용해 나이와 위험도 간 곡선 관계, 성별은 범주 처리
- 신경망은 환자 기록에서 패턴을 추출하여 예측 보정

### 4. 앞으로의 방향: 예측의 다중화, 설명가능성의 회복

#### ① XAI (Explainable AI)와 하이브리드의 결합

- 신경망 단독 모델의 문제: 블랙박스
- 선형 또는 단순 구조와 결합하면: 부분 설명력 확보 가능

#### ② 모델 앙상블의 비선형-선형 혼합화

- Random Forest + Linear Layer → 변수 중요도 해석 가능
- Gradient Boosting + Logistic Regression → 성능 + 해석력

#### ③ Fine-Tuned Control 모델

- 선형 모델로는 정책 기반 의사결정
- 비선형 모델로는 시스템 보정 및 예외 탐지

## 모든 예측은 선형과 비선형 사이에 있다

> “모든 모델은 틀렸다. 그러나 어떤 모델은 유용하다.”  
> — George Box

인간은 예측을 갈망합니다.
미래를 알고 싶어 하는 인간의 본능은 수학과 모델, 기계와 데이터라는 형식을 통해 구체화되어 왔죠.  
그 여정의 출발점에는 선형이 있었고, 그 너머를 향한 도전에는 비선형이 있었습니다.

모든 예측은 선형과 비선형 사이에 있습니다.  
이는 단지 모델 선택의 문제가 아니라,  
그것은 곧 세상을 바라보는 관점,  
불확실성과 함께 살아가는 태도,  
복잡성을 다루는 인간의 지적 여정에 관한 이야기입니다.
