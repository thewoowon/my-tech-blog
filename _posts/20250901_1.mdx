---
title: Attention is Still All You Need?
description: 여전히 Attention이 전부인가?
thumbnail: https://imagedelivery.net/6qzLODAqs2g1LZbVYqtuQw/f08ecef5-63a9-4020-a7be-c8fc54bd4200/public
prerequisites:
  ['우원', 'FE Developer', 'FE Developer', '안녕하세요! 우원입니다.']
stacks: ['ML', 'DL']
writer: 우원
date: '2025-09-01'
name: '20250901_1'
lock: 'false'
---

손끝에서 엔터키를 누르는 순간,
당신이 입력한 프롬프트는 바이너리 신호로 변환되어 서버로 전송됩니다.
그 텍스트는 Tokenizing, Embedding 등의 가공 과정을 거쳐
행렬(Matrix) 형태로 변환되고,
우리가 흔히 LLM (Large Language Model) 이라 부르는 거대한 확률 계산기의 입력으로 사
용됩니다.
몇 번의 아름다운 행렬 곱(Multiplication) 이 이어진 뒤,
우리가 원하는 대답이 스트리밍 통신을 통해
역동적인 형태로 되돌아옵니다.

불과 5년 전만 해도, 우리는 이런 세상을 상상하지 못했습니다.
Transformer는 2017년 논문 “Attention is All You Need” 에서 처음 소개되었지만,
당시엔 단지 기존 RNN/LSTM을 대체할 수 있는 구조 정도로 여겨졌을 뿐입니다.
코로나19 팬데믹으로 혼란스러웠던 시기,
일부 연구집단과 선각자들은 Transformer의 가능성을 눈치챘고,
막대한 GPU 자원과 시간, 돈을 쏟아부으며 그 잠재력을 키워냈습니다.
그러던 중 GPT-3.5, 그리고 GPT-4가 등장했습니다.
비약적인 성능 향상은 단지 모델 크기의 문제가 아니었습니다.
Transformer 아키텍처, 학습 데이터 품질,
GPU의 연산 성능, 메모리 최적화, 소프트웨어 개선…
모든 조건이 하나의 방향을 바라볼 수 있었기에 가능했습니다.
2022년부터는 구글 검색량, 투자, 산업적 관심도
모두가 눈에 띄게 폭발적으로 증가했습니다.
인간의 언어를 이해한다는 것
우리는 과거 ‘심심이’ 같은 룰 기반 챗봇에 익숙했습니다.
정의된 응답 패턴, 문법 규칙에 기반한 단순한 시스템이었죠.
그러나 Transformer 기반 LLM은 달랐습니다.
단어 간의 의미 관계를 수치적 거리와 확률 분포로 포착하고,
심지어 추론(Inference)과 같은 인간의 고차원 사고조차
모델 내부에서 어텐션과 학습된 가중치를 통해 흉내 내는 데 성공했습니다.

2025년 현재,
이 가능성은 단순한 기술의 진보를 넘어
모든 산업을 재편하는 거대한 인식 패러다임으로 작용하고 있습니다.

⚙�Transformer는 어떻게 동작하는가?
Transformer의 흐름은 대략 다음과 같은 단계로 요약할 수 있습니다.
Tokenizing
자연어를 의미 단위로 잘게 쪼개고 숫자로 매핑하는 단계
Embedding + Positional Encoding
토큰을 고차원 공간 벡터로 변환하고, 순서 정보를 부여하는 단계
Self-Attention
입력의 각 단어가 서로를 바라보며 가중치를 계산하는 단계
Scaled Dot-Product Attention
Multi-Head Attention
Feedforward & Normalization
각 위치별 독립적 처리 + LayerNorm + Residual Connection
단어 예측 (Language Modeling)
다음 토큰을 예측하고 확률 분포에 따라 샘플링하는 단계
최종 출력 (Decoding)
스트리밍 또는 완성된 응답 형태로 출력하는 단계
이 아키텍처는 논문에서도 확인할 수 있듯이,
Jakob Uszkoreit(Google Research)가
“replacing RNNs with self‑attention”을 최초로 제안했는데,
그 직관만큼은 리만에 견주어도 손색없다고 생각됩니다.
또한 그는 self‑attention만으로도 sequence 모델링이 가능하며
반복 구조 없이도 높은 성능과 병렬 처리가 가능함을 주장했습니다.
물론 그 주장은 사실이 되었고 “de facto standard” 즉 사실상 표준이라고도 할 수 있습니
다.
이제 앞서 구분한 6단계에 대해 모델 내부에서

어떤 일이 벌어지는지 알아보겠습니다.
[Tokenizing]
우리가 자연어라고 부르는 것은 텍스트일 뿐입니다.
그러나 LLM은 텍스트 자체를 이해하지 못하죠.
이를 이해하기 위해선, 우선 토큰 단위로 나누고, 숫자로 바꿔야 합니다.

여기서 언급하는 토큰은 흔히 ‘형태소’ 정도로 이해하기 보다는
모델이 학습할 수 있도록 만든 텍스트의 최소 단위화 방식으로 받아들여야합니다.
� 토크나이저: 텍스트를 토큰 단위로 분할하고,그 토큰을 정수 ID로 인코딩 혹은 디코딩해주
는 모델
토크나이저는 Byte Pair Encoding (BPE), Unigram, 최근에는 SentencePiece 등 서브워드 기반
의 토크나이저를 사용합니다.
이 원리는 다음과 같습니다.
미리 준비한 토큰사전을 기반으로 가장 작은 단위로 분리합니다.
예를 들어,
"unbelievable" → ["un", "believ", "able"]
이런 식으로 분리할 수 있고,
여기서 "un", "believ", "able”은 토큰사전에 등재되어 있는
가장 작은 단위입니다.
한글도 마찬가지입니다.
"먹었습니다" → ["▁먹", "었", "습니다"]
또는
"안녕하세요" → ["▁안", "녕", "하", "세", "요"]
식으로 분리할 수 있습니다.
(여기서 ‘\_’는 어절 경계를 나타내는 특수 기호)
이제 토큰 단위로 분리하는 과정을 마쳤으니
고유한 정수 ID로 매핑하는 과정이 남았습니다.
하지만 토큰화와 정수 ID 매핑은 사실상 동시에 진행되는데

토큰 자체를 키로써 정수값을 바로 가져올 수 있기 때문입니다.
이제 [“▁안", "녕", "하", "세", “요”]는 최종적으로
[32091, 1278, 1097, 2940, 2010]로 나타낼 수 있습니다.
(정수 ID는 어떤 의미도 없고, 관계도 없으며 단순히 사전 정의된 순서입니다.)
[Embedding + Positional Encoding]
토큰화가 막 끝난 입력은 여전히 의미없는 숫자들의 나열에 불과합니다.
반드시 각 토큰을 고차원 의미 공간에 투영하는 과정을 거쳐야지만
비로소 Attention Mechanism의 재료로 활용될 수 있습니다.
Embedding 차원의 길이는 하이퍼 파라미터로 모델의 크기, 성능, 계산량 trade-off를 고려
해 설계자가 정한 값입니다.
물론 차원이 커질수록 표현력은 좋아지지만 연산도 기하급수적으로 늘어남에 따라 어떻게
설정하는지가 정말 중요합니다.
그렇다면 고차원 공간에 투영한다는 것은 무슨 의미일까요? 그리고 그것은 어떻게 해석하면
될까요?
고차원 임베딩, 즉 Embedding 차원이 크다는 것은 각 단어가 표현되는 벡터 공간이 더 넓
어진다는 것을 의미합니다.
이는 단어 간 의미 차이를 더 정밀하게 잡아낼 수 있다는 말과도 같은데,
흔히 RAG 구축 시 pinecone 같은 벡터 db에서 검색 문장을 깊은 벡터로 만들어 검색하는
것을 생각하면 아주 쉽습니다.
정리하면 학습 파라미터에게 더 깊고 미세한 의미를 인식하게 해주고 결과적으로 확률 분포
의 분해능을 높이는 동시에
다음 토큰 선택 시 더 넓고 세밀한 후보군을 제시하게 만든다는 것이 고차원 공간 투영의
본질입니다.
이제 실제 Embedding을 살펴보면 각 정수 ID를 고정된 크기의 벡터로 변환하는 것과 같습
니다.
예를 들어
토큰 154 → [0.21, -0.58, ..., 0.03] 입니다.
이때 Embedding은 학습 가능한 파라미터이고 의미적 거리를 담고 있습니다.
마치 강아지와 고양이는 벡터 공간에서 가까이 있는 것과 같죠.
(의미상 강아지와 고양이는 반려동물로 대등한 선에서 비교되거나 공통점을 가지기 때문.)

​<수식>
순서 기반 구조인 RNN/LSTM 입력 시퀀스를 왼쪽부터 오른쪽으로 한 토큰씩 차례대로 처리
하면서,
이전 시점의 hidden state를 다음 시점으로 넘겨주기 때문에, 순서에 따라 처리되는 정보가
누적되어 갑니다.
예를 들어 문장이 이렇게 있다고 가정하면,
Input: ["나는", "학교에", "간다"]
첫 번째 step: "나는" → hidden state h₁
두 번째 step: "학교에" + h₁ → h₂
세 번째 step: "간다" + h₂ → h₃
이렇게 되면 LSTM은 "간다"를 처리할 때, 이미 "나는", "학교에"의 정보를 담은 상태인 h₂를
받게되고,
즉, "간다"가 세 번째 토큰이라는 정보는 순서를 통해 자연스럽게 인코딩됩니다.
하지만 Transformer는 이런 순서 정보를 알 수 없기 때문에
입력의 위치 정보를 직접 추가해줘야 하는데,
그 방법이 바로 Positional Encoding입니다.
그 결과 "나는 간다 학교에"와 "학교에 나는 간다"를 구분할 수 있게 되는 것이죠.
수식은 다음과 같습니다.
<수식>
pos는 0부터 시작하는 토큰의 위치이고,
i는 차원의 인덱스 (0, 1, ..., d_model/2 - 1)입니다.
또 여기서 d_model은 임베딩 차원 수(예: 512)로
짝수 차원은 sin, 홀수 차원은 cos을 사용합니다.
정리하면 단어(토큰) 임베딩에 위치 인코딩을 더한 것이 현재 구간의 최종 결과물입니다.
x = word_embedding + positional_encoding
+) Learned Positional Encoding, RoPE (Rotary Positional Embedding) 같은 다른 방식들도
존재합니다.
[Self-Attention]

Self-Attention은 입력 시퀀스의 모든 단어쌍 간 관계를 동시적으로 고려하여,
각 토큰이 전체 문맥에서 어떤 정보에 주목해야 하는지를 학습하는 메커니즘입니다.
+) FFN과 NormLayer와 함께 Self-Attention Block을 구성하지만, 연산량이 기하급수적으로
늘어나는 원인이기도 합니다.
이 구조는 Recurrent 구조 없이도 문맥을 포착할 수 있게 하고
병렬화가 가능한 구조를 제공한다는 점에서 Transformer 아키텍처의 핵심이라 할 수 있습니
다.
▶ Scaled Dot-Product Attention
Self-Attention은 Query $Q$, Key $K$, Value $V$ 세 행렬을 기반으로 작동하고
이 세 행렬은 입력 $X \in \mathbb{R}^{n \times d}$ 에 대해 다음과 같이 계산됩니다.

$$
Q = XW^Q, \quad K = XW^K, \quad V = XW^V
$$

※ 여기서 $W^Q, W^K, W^V \in \mathbb{R}^{d \times d_k}$ 는 학습 가능한 가중치
행렬
이제 Q, K, V의 행렬을 활용한 Scaled Dot-Product Attention 출력은 다음과 같이 정의됩니
다.

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

- $QK^\top$: Query와 Key 간의 유사도(내적)를 통해 attention score를 계산
- $\sqrt{d_k}$: 점곱의 스케일을 조정하여 gradient vanishing을 방지
- softmax: attention score를 확률 분포로 변환
- $V$: 최종적으로 결합될 정보 벡터
  이 구조는 입력 시퀀스 내의 각 토큰이 다른 모든 토큰에 대해 주의를 얼마나 기울여야 하
  는지를 동적으로 결정합니다.
  "입력 시퀀스 내의 각 토큰이 다른 모든 토큰에 대해 주의를 얼마나 기울여야 하는지"를 조
  금 더 풀어서 말해보면.
  "문맥 안에서 중요한 단어에 집중해라"로 해석할 수 있습니다.

▶ Multi-Head Attention
Multi-Head Attention은 "Scaled Dot-Product Attention"의 선행 과정이라고 볼 수 있습니
다.
Embedding 벡터를 여러 개의 attention head로 나누고, 각 헤드가 독립적으로 Scaled
Dot-Product Attention을 계산한 뒤,
결과를 다시 합쳐서 더 풍부한 표현력을 얻기 때문입니다.
※ attention head 수는 고정된 숫자.
물론 여러개의 attention head로 나누는 이유도 단일 attention head의 한계와 제한된 표현
력 때문입니다.
그렇기에 각각의 attention head는 서로 다른 $W^Q_i, W^K_i, W^V_i$ 파라미터를 가지고,
서로 다른 attention map을 학습합니다.

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(head_1, ..., head_h)W^O
$$

$$
\text{where} \quad head_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)
$$

)+ 여기서 $W^O \in \mathbb{R}^{hd_v \times d}$ 는 출력 차원을 맞추기 위한 선형 변
환 행렬입니다.
이를 통해 모델은 다양한 의미적 관계를 병렬적으로 파악할 수 있습니다.
[Feedforward & Normalization]
Attention layer를 통과한 출력은 각 위치별로 독립적인 Position-wise Feedforward Network
(FFN) 를 거칩니다.
이는 다음과 같이 정의됩니다.

$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

여기서 $W_1 \in \mathbb{R}^{d \times d_{ff}}, W_2 \in \mathbb{R}^{d_{ff} \times d}$
는 학습 가능한 가중치이며,
$d_{ff} \gg d$ 인 경우가 일반적입니다. (예: $d = 512, d_{ff} = 2048$).

Feed Forward Network (FFN)는 하나의 토큰 벡터를 받아서 비선형 변환을 수행하는 작은
MLP (다층 퍼셉트론)으로 정의할 수 있습니다.
즉, Self-Attention이 문맥을 모아주면 FFN은 그걸 개별적으로 가공해서 의미를 강화하는 처
리기 역할로 볼 수 있습니다.
이후 Transformer는 안정적인 학습을 위해 각 sub-layer (Multi-head attention, FFN) 뒤에
Residual Connection과 Layer Normalization을 적용합니다.

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

Residual Connection은 기울기 흐름을 원활하게 하여 deep structure에서 학습 효율성을 높
이고,
Layer Normalization은 내부 공변량 변화(internal covariate shift)를 완화할 목적으로 사용합
니다.

[단어 예측: Language Modeling Head]
Transformer의 출력은 각 시점 $t$에서 다음 토큰을 예측하는 언어 모델링 태스크로 이어집
니다.
즉, 확률 분포 $P(w_t \mid w_{<t})$ 를 모델링하는 문제로 귀결됩니다.
최종 출력 벡터 $z_t$는 학습된 출력 임베딩 행렬 $W_E \in \mathbb{R}^{V \times d}$
(일반적으로 입력 임베딩 행렬을 공유함)와의
내적을 통해 각 단어에 대한 로짓(logit)을 계산하고, softmax라는 비선형함수를 통해 확률
분포로 변환됩니다.

$$
P(w_t \mid w_{<t}) = \text{softmax}(z_t W_E^\top + b)
$$

여기서 $V$는 어휘 사전의 크기, $d$는 임베딩 차원입니다.
+) 이때 손실 함수로는 Cross Entropy 사용.

$$
\mathcal{L} = - \sum_{t=1}^T \log P(w_t \mid w_{<t})
$$

[최종 출력: Decoding]
LLM은 자기회귀(autoregressive) 방식으로 동작하며,
이전까지 생성된 토큰을 기반으로 다음 토큰을 순차적으로 예측합니다.
생성 과정은 크게 다음과 같은 decoding strategy를 따릅니다.

1. Greedy decoding: 가장 확률 높은 토큰 선택
2. Beam search: 여러 후보 경로를 유지하며 탐색
3. Top-k sampling: 확률 상위 k개 중 샘플링
4. Top-p (nucleus) sampling: 누적 확률 p 이상이 되는 후보 집합에서 샘플링
   이를 통해 생성된 토큰 시퀀스는 다음과 같이 출력됩니다.
   $$
   w_1 \to w_2 \to \cdots \to w_T
   $$
   일반적인 GPT 구조에서는 출력이 실시간 스트리밍 방식으로 사용자에게 제공됩니다.
   즉, 내부적으로 디코딩이 진행되는 동시에, 가장 먼저 확정된 토큰부터 순차적으로 출력 버
   퍼를 통해 전송되는 것입니다.
   +) 단어 chunk를 실시간으로 이어 받아 writer 효과
   +) 가장 높은 확률의 단어를 전송하는 Transformer의 메커니즘을 추측해볼 수 있음.
   � 마무리
   최종적으로 Transformer의 내부 작동 방식은 다음과 같은 추상화된 수식 구조로 정리해 볼
   수 있습니다.
   $$
   X
   $$

\xrightarrow{\text{Tokenize}}

\xrightarrow{\text{Positional}}

\{w_1,
E'

...,

w_T\}

\xrightarrow{\text{Embedding}}

\xrightarrow{\text{Multi-Head

Attention}}

E
A

\xrightarrow{\text{FFN + Norm}} H \xrightarrow{\text{Output Head}} P(w\_{t+1})

$$
우리가 간편하게만 사용하던 LLM의 놀라도록 치밀하고 혁명적인 구조를 조금이나마 알게
되셨다면 좋겠습니다.
감사합니다.

Transformer에 대한 몇 가지 재미있는 사실들
1. GPT-3 175B 모델의 추론 한 번당 약 350 GFLOPs/token, 즉 100개의 token으로 이루어
진 답을 만들기 위해 약 35 TFLOPs 필요
2.

현재

400B급

모델도

8

Blackwell

GPU

기반

DGX

B200

구조에서는

≈1,000

tokens/sec/user 수준의 실시간 대응이 가능
3. LLM은 수학 문제를 푸는 것처럼 보이지만, 사실 대부분은 확률적으로 가장 그럴듯한 다
음 단어를 예측하는 것.
4. GPT-3를 한 번 학습시키는 데 약 1287 MWh가 소모되며, 이는 전기차 테슬라 모델3 기
준 약 12만 km 주행 가능한 전력.
5. 엔비디아 H100 GPU는 FP8 기준 약 4 PFLOPS, B200은 9 PFLOPS의 dense 연산 성능을
자랑, 이는 1초에 9000조 번의 부동소수점 계산이 가능한 수준
※ FLOPs -> Floating Point Operations
※ 1 GFLOP = 10억 개의 부동소수점 연산 / 초
※ 1 TFLOP = 1조 개의 부동소수점 연산 / 초
※ A100 기준 20 TFLOPS (float32) ~ 300+ TFLOPS (float16, tensor core) 까지 가능


$$
