---
title: 복잡도는 운명인가? P-NP와 시스템 전반의 한계
description: 복잡도 이론의 근본 질문, P vs NP 문제는 단순한 이론적 호기심이 아닙니다.
thumbnail: https://imagedelivery.net/6qzLODAqs2g1LZbVYqtuQw/86f58168-b2b9-41c6-7132-f204a831f500/public
prerequisites:
  ['우원', 'FE Developer', 'FE Developer', '안녕하세요! 우원입니다.']
stacks: ['MATH']
writer: 우원
date: '2025-09-30'
name: '20250930_1'
lock: 'false'
---

# 1장. 자연과 코드의 공통 질문

고대 수학자들이 처음으로 방정식을 세웠을 때부터 지금까지,
인간은 “세상의 문제를 계산으로 풀 수 있는가”라는 질문을 놓지 않았습니다.
뉴턴의 운동 방정식, 맥스웰의 전자기 방정식, 슈뢰딩거의 파동 방정식 등이 자연의 복잡한 현상을 공식으로 담아내려 했던 시도이자 바로 그 증거입니다.
하지만, 역설적이게도, 공식이 있다고 해서 우리가 그 문제를 실제로 풀 수 있다는 뜻은 아닙니다.

예를 들어, 유체역학의 기본 방정식인 나비에–스토크스 방정식은 200년 가까이 연구되었지만,
여전히 해석해를 찾는 것은 난제로 남아 있습니다.
우리가 날씨를 정확히 예측하지 못하는 이유도, 근본적으로 이 방정식이 갖는 계산 복잡성 때문입니다.

생물학에서도 마찬가지입니다.
단백질의 아미노산 서열이 주어졌을 때,
그것이 어떻게 접히는지 예측하는 문제는 단순히 풀 수 있다고 말하기 어렵습니다.
가능한 접힘의 경우의 수가 천문학적으로 폭발하기 때문에,
이 문제는 전형적인 NP-hard 문제의 예시로 꼽히죠.
실험적 방법 없이, 계산만으로 단백질 구조를 정확히 예측하는 것은 사실상 불가능에 가깝다는 말과 같습니다.

이처럼 “풀 수 있는 문제”와 “계산적으로 tractable한 문제”는 다릅니다.
바로 여기서 컴퓨터 과학의 근본 질문, P vs NP 문제가 등장하게 됩니다.

- P (Polynomial time): 다항시간 안에 풀 수 있는 문제
- NP (Nondeterministic Polynomial time): 답이 주어졌을 때, 다항시간 안에 검증할 수 있는 문제

이 구분은 보면 단순히 이론적 장난으로 보일 수 있습니다만..
이것은 엄연한 정의로서 복장성 이론체계에서 완전성을 보장합니다.
실제로 우리가 프로그래밍을 하면서 직면하는 수많은 난제들을 나열해보죠.
데이터베이스 최적화, 분산 시스템의 자원 배치, AI 학습 알고리즘 등
말만 들어도 머리가 아픈 문제들의 기저에는 이미 NP의 그림자가 짙게 드리워져 있습니다.

따라서 P vs NP 문제는 수학자들만의 추상적 유희가 아니라,
자연과 기술 전반에 걸친 ‘운명적인 질문’인 셈입니다.

※ P와 NP에 대한 정의를 이해하셨지만, 추가로 '다항시간'이 의미하는 바가 무엇인지 궁금한 분들이 계실겁니다.
다항시간(polynomial time)이란, 입력 크기 n에 대하여 알고리즘의 수행 시간이 𝑂(𝑛^𝑘) 꼴로
어떤 상수 𝑘에 대해 상한 지어질 수 있는 경우를 의미합니다.
즉, 실행 시간이 지수적(2^𝑛, 𝑛!)이거나 초다항적(2^(n^(1/2)) 등)인 것이 아니라,
다항식의 범주 안에 포함되는 경우를 말합니다.

# 2장. 복잡도와 데이터 (DA, AI)

데이터 분석과 인공지능은 현대 사회에서 가장 “계산 친화적”인 분야처럼 보입니다.
수많은 데이터가 수집되고, 정제되고, 모델로 학습되며, 그 결과는 비즈니스와 과학 전반에 적용되죠
그러나 그 내부를 들여다보면, 이 역시 복잡도의 장벽과 끊임없이 씨름하고 있음을 알 수 있습니다.

## 2.1 학습 문제의 본질: 최적화

딥러닝 모델을 학습시킨다는 것은 결국 손실 함수(loss function)를 최소화하는 문제입니다.
그러나 이 문제는 일반적으로 비선형, 비볼록이며, 이론적으로는 NP-hard에 가깝습니다.
즉, 전역 최적해를 보장하는 다항시간 알고리즘은 존재하지 않는다는 것입니다.

※ 비선형에 대한 내용은 이전 글을 확인해주세요! 비볼록(non-convex)는 짚고 넘어가지 않겠습니다.

따라서 우리가 실제로 사용하는 것은 근사적 방법(heuristics)입니다.
확률적 경사하강법(Stochastic Gradient Descent, SGD)이나 Adam 같은 최적화 알고리즘은 본질적으로
“빠르게 수렴하는 근사해”를 제공할 뿐이죠.
즉, 딥러닝은 거대한 연산 능력을 동원해도 정확한 해를 구하는 것이 아니라, 충분히 쓸 만한 근사해를 찾는 과정이라고 볼 수 있습니다.

※ 머릿속에 떠올리시는 휴리스틱은 일반적 의미(심리학/철학/인지과학)에 가까울겁니다.
즉 빠른 결정을 내리기 위해 쓰는 간편한 추론법 말이죠.
하지만 컴퓨터 과학/알고리즘 설계에서는 어떤 문제, 특히 NP-hard 문제처럼 최적해를
다항시간 안에 구할 수 없는 것으로 여겨지는 문제에 대해,
정확한 최적해는 보장하지 않지만 실용적으로 '좋은'해를 빠르게 찾는 방법을 뜻합니다.

예: 휴리스틱 탐색(A\*, Hill climbing 등)

$$
\min_{\theta} L(\theta; X, y)
$$

$$
\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t; X_b, y_b)
$$

이 단순한 수식 뒤에는 “NP-hard 문제를 현실적으로 다루는 인간의 방식”이 숨어 있습니다.

## 2.2 몬티홀 문제와 데이터 직관

데이터 과학에서 중요한 것은 직관과 확률의 간극입니다.
대표적인 예가 몬티홀 문제이죠.

나의 직관과 확률적 간극이 얼마나 떨어져 있는지 한 번 테스트해보세요.
지적유희로도 아주 좋은 문제입니다!

문제는 다음과 같습니다.

세 개의 문 중 하나에 자동차가 있고, 나머지 두 개에는 염소가 있다고 합시다.
당신은 참가자로서 문 하나를 고른 뒤, 내심 문 너머에 고급 세단이 있기를 바라고 있죠.
그런데 감자기 사회자가 염소가 있는 다른 문 하나를 열고, 다시 선택을 바꾸도록 제안합니다.
이 때 당신은 선택을 바꾸시겠습니까?

바꾼다면 바꾸는게 유리한 이유, 바꾸지 않는다면 바꾸지 않는게 유리한 이유를 말해보세요.

=================================================================================

생각의 시간....

=================================================================================

정답은 "바꿔야 한다"입니다!

많은 사람들은 사회자가 선택을 바꿀 것을 제안했을 때,
“어차피 두 개 중 하나니까 확률은 50:50”이라고 생각합니다.
하지만 실제 확률은 처음 고른 문이 맞을 확률은 1/3, 바꿨을 때 맞을 확률 2/3이죠.

이것은 처음 선택의 경우를 나누어 생각해보면 쉽습니다.

[처음] 세단을 고른 경우 -> 바꿀 이유 없음

[처음] 세단을 고르지 않은 경우(염소) -> 하나를 공개(염소) -> 나머지는 무조건 세단 -> 무조건 바꿔야 함

이 역설적인 결과는, 인간의 직관이 얼마나 계산적 복잡성을 단순화하려는 경향이 있는지 보여줍니다.
데이터 분석과 인공지능에서도 마찬가지입니다.
직관은 우리를 속이고, 계산은 현실을 드러내죠.
그러나 계산조차 NP-hard 문제 앞에서는 한계에 부딪히고 맙니다.

## 2.3 데이터베이스와 복잡도

데이터 분석에서의 복잡도는 AI 학습에만 국한되지 않습니다.
예를 들어, SQL 쿼리 최적화에서 가장 중요한 문제 중 하나는 조인 순서를 결정하는 것입니다.
그러나 최적의 조인 순서를 찾는 문제는 사실상 NP-hard로 알려져 있죠.
데이터베이스 옵티마이저는 완전탐색을 수행하는 대신, 동적 계획법, 휴리스틱, 비용 추정 기반 탐색을 사용합니다.

따라서 대규모 빅데이터 환경에서 “쿼리가 느리다”는 것은 단순한 기술적 문제가 아니라,
숨어 있던 NP-hard 문제의 그림자가 드러나는 순간이라 할 수 있습니다.

## 2.4 데이터 과학자의 태도

데이터 과학자가 데이터를 다룬다는 것은, 단순히 수학적 계산을 넘어서
“풀 수 없는 문제를 어떻게 근사적으로 풀어낼 것인가”라는 태도를 의미합니다.
정확한 모델보다 빠르고 실용적인 모델, 완벽한 쿼리보다 충분히 효율적인 쿼리를 선택하는 것은
바로 복잡도의 본질을 체화한 선택으로 볼 수 있습니다.

# 3장. 복잡도와 프론트엔드 (FE)

프론트엔드 개발은 흔히 "사용자 경험(UX)을 위한 인터페이스 작업" 정도로 간주됩니다.
하지만 그 이면에는 의외로 깊은 계산 복잡성 문제가 숨어 있다는 것을 아시나요?

이와 비슷하게 브라우저도 단순히 HTML과 CSS, 자바스크립트를 해석하는 도구로 취급하지만
알고보면 실시간 최적화 문제 해결기에 가깝다는 것을 이번 시간을 통해 한 번 짚고 넘어가셨으면 합니다.

## 3.1 브라우저 렌더링 파이프라인과 계산의 무게

브라우저가 한 페이지를 표시하기 위해 수행하는 과정은 우선 다음과 같습니다.

1. HTML 파싱 → DOM 트리 구축
2. CSS 파싱 → 스타일 규칙 적용 → CSSOM 트리 생성
3. DOM + CSSOM → 렌더 트리(Render Tree)
4. 레이아웃(Layout): 각 요소의 위치와 크기 계산
5. 페인팅(Painting): 픽셀로 색상과 이미지 렌더링
6. 합성(Compositing): 레이어를 GPU에 넘겨 화면에 표시

이 중 레이아웃 계산과 스타일 매칭은 계산 비용이 기하급수적으로 늘어날 수 있는 핵심 구간입니다.
특히 레이아웃은 트리 구조 전체에 전파되며,
최악의 경우 $O(n)$ \~ $O(n \cdot m)$ (n=노드 수, m=스타일 규칙 수)의 비용이 발생합니다.

## 3.2 CSS 선택자 매칭의 복잡성

CSS 선택자(selector) 매칭 문제는 단순해 보이지만,
특정 경우에는 NP-hard에 준하는 복잡성을 가집니다.

- 단일 선택자 (예: `.class`): $O(1)$ 또는 $O(n)$
- 후손 선택자 (예: `div ul li a`): 트리 탐색으로 인해 $O(n \cdot d)$, d는 트리 깊이
- 조합 선택자 및 부정 조건 (예: `a[href*="news"]:not(.highlight)`): 부분집합 검증과 유사, $O(2^n)$까지 폭발 가능

즉, 최악의 경우 CSS 매칭은 SAT(충족 가능성 문제)와 비슷한 구조를 가집니다.
(실제로 이론 연구에서도, CSS 선택자의 충돌 검사나 최적 적용 순서 문제는 NP-complete로 분류될 수 있음을 보였습니다.)

## 3.3 React와 Virtual DOM: Diffing의 복잡도

React의 Virtual DOM이 해결하려는 핵심은 DOM diffing 최소화 문제입니다.

다음과 같은 문제가 있다고 가정해봅시다.

"두 트리 $T_1$, $T_2$가 주어졌을 때, 최소 편집 거리를 구하라!"

(자료구조나 알고리즘에 관심이 있으시다면...)
이 문제는 흔히 트리 편집 거리 문제로 알려져 있는데,
일반적으로 $O(n^3)$의 시간 복잡도를 가집니다.
또한 최적화된 알고리즘이 존재하더라도,
최소 차이 계산(minimal diff) 자체는 NP-hard한 변형으로 귀결되죠.

따라서 React는 완벽한 최소 diff 대신,
근사 알고리즘 즉 heuristics을 선택했습니다.
예를 들어 key 속성을 통해 노드를 식별하는 전략을 취하는데,
이는 탐색 공간을 지수에서 선형으로 줄이는 방법입니다!

정리해보면 Virtual DOM은 단순한 라이브러리 기능이 아니라,
NP-hard 문제를 현실적으로 tractable하게 만든 근사화 전략이라고 보는게 맞습니다.

## 3.4 애니메이션과 60fps 제약

프론트엔드의 성능에서 자주 언급되는 기준이 초당 60프레임(60fps)입니다.
이는 곧 한 프레임당 16.67ms의 계산 시간이 주어진다는 뜻으로 해석할 수 있습니다.
만약 레이아웃 계산이 $O(n^2)$, diffing이 $O(n^3)$라면,
요소 개수 $n$이 1,000개를 넘어가는 순간 실시간성은 이미 붕괴되었다고 보면 되죠.

따라서 브라우저와 프레임워크는 완벽한 최적화 대신 근사적, 경험적 최적화를 택하게 됩니다.
(필수불가결한 선택일지도...)
이는 전형적인 실시간 근사화 문제로 볼 수도 있는데,
알고리즘 복잡도와 사용자 경험(UX) 사이의 트레이드오프가 수반할 수 밖에 없습니다.

---

## 3.5 요약

프론트엔드 개발은 단순히 "보이는 것"을 만드는 일만이 아닙니다.
그 이면에는 NP-hard 문제를 근사적으로 다루는 전략들이 숨어 있죠.
CSS 선택자 매칭, DOM diffing, 애니메이션 최적화는 모두 복잡도와 긴밀히 맞닿아 있고
우리는 그 사실을 잊은 채 브라우저가 매 순간 NP와 싸우고 있다는 것을 당연하게 생각합니다.

---

# 4장. 복잡도와 백엔드 (BE)

백엔드 개발은 흔히 "비즈니스 로직"이나 "데이터 관리"로 단순화되지만,
그 내부를 자세히 들여다보면 조합의 폭발(?) 혹은 발산과 맞서는 끊임없는 싸움의 연속입니다.
요청 스케줄링, DB 쿼리 최적화, 분산 트랜잭션 관리 등은 모두 이론적으로 NP-hard 혹은 그에 준하는 복잡성을 가지고 있습니다.

---

## 4.1 요청 스케줄링과 NP-hard

서버에 들어오는 요청을 최소 지연과 최대 처리량으로 스케줄링한다고 해보겠습니다.
이 때부터 스케쥴링은 단순 시퀀스 처리나 큐잉으로 해결되는 수준이 아니라
전형적인 "작업 스케쥴링 문제"로 귀결되고, 일반적으로 NP-hard로 간주됩니다.

$$
\min_{\pi \in S_n} \sum_{i=1}^{n} w_i \cdot C_i(\pi)
$$

여기서 $\pi$는 요청들의 순열, $C_i(\pi)$는 i번째 요청의 완료 시간입니다.
이 문제는 가중완료시간 스케쥴링으로 알려져 있고 NP-hard입니다.

※ 가중 완료 시간(WCT) 스케줄링은 각 작업의 가중치에 완료 시간을 곱한 값의 합을 최소화 하는 방향으로 처리할 작업 순서를 결정하는 최적화 문제입니다!

하나의 사례로 Google Borg를 예로 들 수 있습니다.
구글의 클러스터 관리 시스템 Borg(쿠버네티스의 전신)는
작업 스케쥴링의 문제를 풀기 위해 탄생했지만,
“최적 스케줄링”은 불가능하다는 것을 쿨하게 인정했고,
대신 우선순위 큐 + 휴리스틱 기반 자원 할당을 사용했습니다.
즉, 세계 최대 규모의 데이터센터도 NP-hard 문제의 근사해를 현실적으로 취할 수밖에 없는 것입니다.

---

## 4.2 DB 쿼리 최적화와 조합 폭발

데이터베이스 옵티마이저의 핵심은 조인 순서(join order)를 찾는 것입니다.
(SQLD 수준에서 Nested Loop Join, Hash Join, Sort-Merge Join 많이 들어보셨죠?)
가능한 조인 순서의 수는 Catalan 수로 폭발한다:

※ SQL 옵티마이저에서 조인 순서(join order) 가 폭발적으로 많아지는 걸 설명할 때,
흔히 Catalan 수(Catalan Number)를 가져옵니다.

Catalan 수는 조합론에서 자주 등장하는 특수 수열로,
“어떤 구조를 만들 수 있는 경우의 수”를 셀 때 자주 씁니다.

공식은 이렇습니다!

[
C_n = \frac{1}{n+1}\binom{2n}{n} = \frac{(2n)!}{(n+1)! , n!}
]

그리고 첫 몇 항은 다음과 같습니다!

[
C_0=1, ; C_1=1, ; C_2=2, ; C_3=5, ; C_4=14, ; C_5=42, ; C_6=132, \dots
]

---

$$
C_{n-1} = \frac{1}{n} \binom{2n-2}{n-1}
$$

예:

- $n=5 \Rightarrow 14$
- $n=10 \Rightarrow 4,862$
- $n=15 \Rightarrow 9,694,845$

즉, 테이블 수가 조금만 늘어나도 순식간에 탐색 불가능이 됩니다.

하나의 사례로 PostgreSQL 옵티마이저를 예로 들 수 있습니다.
PostgreSQL은 조인 순서 최적화를 위해 동적 계획법(DP)과 휴리스틱을 혼합했습니다.
테이블 수가 12\~15개를 넘어가면 완전 탐색은 포기하고, 비용 기반 근사 탐색으로 전환하는 것이 포인트입니다.
즉, 우리가 실행하는 `SELECT` 한 줄은 사실상 NP-hard 문제에 대한 절충적 탐색 결과인 셈이죠.

---

## 4.3 분산 트랜잭션과 CAP 정리

분산 환경에서는 더 근본적인 제약이 드러납니다.
CAP 정리에 따르면, 네트워크 분할이 발생했을 때
일관성(Consistency), 가용성(Availability)을 동시에 만족할 수 없습니다.
(마치 볼츠만의 불확정성의 원리와 비슷하지 않나요?
속도와 위치를 동시에 정확히 측정할 수 없다는 부분에서 말이죠..)

$$
\forall r \in R, \quad (C(r) \land A(r)) \not\Rightarrow P(r)
$$

이것은 단순한 제약이 아니라, 근본적 불가능성으로 풀이됩니다.

하나의 좋은 사례로 DynamoDB와 MongoDB를 들여다 보죠.
Amazon DynamoDB는 AP 보장 위주로 설계되었고,
이에 따라 네트워크 분할 시에도 응답을 주긴 하지만 일관성을 일시적으로 포기하게 됩니다.
반대로 MongoDB는 Config에 따라 CP 또는 AP에 치우칠 수 있게 설계되었는데,
레플리카셋의 majority write concern이 CP를 지향하는 것을 보면 알 수 있습니다.

즉, 거대 산업 시스템조차 CAP의 제약을 피해갈 수 없고
결국 "어떤 성질을 포기할 것인가?"라는 근사적 선택을 하게 된다고 볼 수 있습니다.

---

## 4.4 백엔드와 근사적 해법

백엔드 엔지니어가 마주하는 대부분의 문제는 본질적으로 완전해를 구할 수 없는 상황입니다.
따라서 현실적 시스템은 항상 근사적 전략을 취하게 돼죠.

- 스케줄링 → Borg, Kubernetes: 우선순위 기반 + 휴리스틱 (근사해)
- 쿼리 최적화 → PostgreSQL 옵티마이저: DP + 비용 추정 (근사해)
- 분산 시스템 → DynamoDB(AP), Zookeeper(CP): CAP 제약 속 절충 (근사해)

즉, 백엔드는 NP-hard 문제와 불가능 정리의 교차점 위에서,
항상 “현실적인 근사해”를 찾아가는 학문(..!)이라 할 수 있습니다.

---

## 4.5 요약

백엔드는 더이상 단순한 로직 구현으로 보지 않는게 맞습니다.
스케줄링의 NP-hard성, 조인 순서의 조합 폭발, CAP 정리의 불가능성 등이 곳곳에 산재해 있기 때문이죠.
구글, 아마존, 오픈소스 DBMS와 같은 거대 시스템조차
완전해 대신 근사해를 택하는 것을 확인 할 수 있었습니다.
결론적으로 백엔드는 수학적 난제와 현실적 제약의 타협 지점이고
모든 설계와 구현은 이 타협 위에서 이루어지게 되는 것입니다.

---

# 5장. 복잡도와 아키텍처 엔지니어링 (AE)

아키텍처 설계는 “서비스를 잘 돌리는 기술”로 보이지만,
실제로는 조합최적화와 정보이론적 한계를 다루는 공학입니다.
클라우드 리소스 배치, 네트워크 라우팅, 관찰가능성(Observability)
모두가 NP-hard 혹은 불가능성 정리의 경계에서 운영됩니다.

---

## 5.1 클라우드 리소스 배치: Bin Packing/Knapsack의 얼굴

Multi-tenant 클러스터에 워크로드를 얹는 과제는 전형적인 Bin Packing 문제입니다.
가령 각 워크로드 $i$가 $(\text{cpu}_i, \text{mem}_i, \text{io}_i)$ 요구량을 갖고, 노드 $j$의 용량이 $(C^{\text{cpu}}_j, C^{\text{mem}}_j, C^{\text{io}}_j)$일 때를 식으로 나타내면..

※ Bin Packing: 냅색과 비슷하지만, 가방을 여러개 쓸 수 있고 공들을 다 집어넣기 위한 가장 최소 가방 개수를 구하는 문제 -> 분할 문제입니다!
※ 워크로드: 시스템, 서비스 또는 애플리케이션이 실행하는 작업, 프로세스 또는 작업

$$
\text{find } x_{ij}\in\{0,1\}\quad
\text{s.t.}\;\sum_i x_{ij}\,\text{cpu}_i \le C^{\text{cpu}}_j,\;
\sum_i x_{ij}\,\text{mem}_i \le C^{\text{mem}}_j,\;
\sum_i x_{ij}\,\text{io}_i \le C^{\text{io}}_j
$$

이 때 목표는 비용 $\text{Cost}=\sum_j \alpha_j \cdot \mathbf{1}[\text{node }j\text{ used}]$ 최소화 또는 SLO(지연/가용성) 제약 하에서의 성능 최대화입니다.
(지나가는 사실 또는 사례 정도로만 파악하고 넘어가세요!!)

조금 더 분류를 좁혀보면 다차원(2차원 이상) Bin Packing/Knapsack 문제로 볼 수 있고
이런 부류의 문제는 NP-hard로 알려져 있습니다.
그렇기 때문에 대규모 시스템에서 근사/휴리스틱이 필수로 요구됩니다.

이번에도 실제 사례를 들어 AWS/GCP의 스케줄링이 어떻게 달라지는지 알아보죠.
Kubernetes 스케줄러는 기본적으로 Score/Filter 기반 휴리스틱(예: LeastRequested, BalancedAllocation, NodeAffinity 등)으로 탐색공간을 줄이고 있고,
GKE Autopilot 및 AWS Karpenter는 실시간 가격·가용존·인스턴스 패밀리(예: c, m, r 계열) 제약까지 얽혀서 Online Bin Packing으로 변환되기 때문에 Preemptible/Spot을 섞을 때는 기대 중단비용까지 포함한 확률적 목표함수를 쓰는 것이 관례로 취급됩니다.

결론적으로 “최적 배치”는 실무에 개념으로만 존재하고 찾을 수 없습니다.
그런 이유로 충분히 좋은 배치를 빠르게 찾는 것이 목표로 동작합니다.

---

## 5.2 네트워크 라우팅: P에서 NP로 넘어가는 지점

단일 소스 최단경로 즉 우리가 흔히 아는 다익스트라 알고리즘은 $O(E\log V)$로 P에 속합니다.
그러나 멀티제약(Multi-Constraint) 라우팅—예를 들어, 지연(latency) $\le L$, 대역폭 $\ge B$, 손실률 $\le p$—을 동시에 만족하는 경로를 찾는 문제는 일반적으로 NP-hard로 알려져 있다.

형식화:

$$
\min_{P\in\mathcal{P}(s,t)} \sum_{e\in P} w_e
\quad \text{s.t.}\;\sum_{e\in P}\ell_e \le L,\;\min_{e\in P}\text{bw}_e \ge B,\;\sum_{e\in P}\rho_e \le p
$$

제약이 늘어날수록 탐색공간은 지수적으로 커지고, 정적 최단경로는 실제 트래픽 변동(혼잡, 장애, 토폴로지 변화)로 금세 비최적화된다.

실제 사례 – GCP/L4LB, Anycast/CDN

- GCP 내부 라우팅·L4 Load Balancer는 ECMP + 근사적 비용함수로 빠른 재수렴을 택한다.
- CDN/Anycast는 최단거리 대신 관측 기반(실측 RTT, 실패율)으로 동적 가중치를 갱신하는데, 이는 사실상 Online Optimization + Bandit에 가깝다.
  결론적으로, 네트워킹도 “한 방의 최적해”가 아닌 지속적 근사가 기본 전략이다.

---

## 5.3 Observability: 정보이론과 샘플링의 벽

시스템을 100% 관찰하고 싶지만, 샘플링·집계·전송 비용이 병목이다. 관찰가능성은 종종 통계적 추정 문제로 환원된다.

### 5.3.1 추정 오차의 하한 (Cramér–Rao Bound)

모수 $\theta$를 추정할 때, 어떤 불편추정량 $\hat{\theta}$의 분산은 피셔 정보 $I(\theta)$로 아래와 같은 하한을 갖는다:

$$
\mathrm{Var}(\hat{\theta}) \ge \frac{1}{I(\theta)}
$$

즉, 관측량(샘플 수)을 늘리지 않고는 특정 정밀도 이하로 내려갈 수 없다.

### 5.3.2 스케치/요약 자료구조의 근사성

- Count-Min Sketch: 주파수 추정 오차 $\varepsilon$를 얻으려면 폭 $w \approx e/\varepsilon$, 실패확률 $\delta$를 억제하려면 깊이 $d \approx \ln(1/\delta)$ 필요. 메모리와 정확도 간 명시적 트레이드오프.
- HyperLogLog: 고유 개수 추정의 표준오차 $\approx 1.04/\sqrt{m}$ (레지스터 $m$). 즉, 정확도를 두 배로 개선하려면 4배의 메모리가 든다.

실제 사례 – AWS/GCP 모니터링

- CloudWatch / Cloud Monitoring는 지표를 다운샘플링·롤업한다. 고해상도(예: 1s)로 오래 보존하려면 비용과 저장 한계가 기하급수적으로 증가.
- 대규모 분산 트레이싱(예: OpenTelemetry)은 헤드/테일 샘플링으로 비용을 억제—결국 근사 추정을 전제로 한다.

> 결론: “모든 것을 본다”는 목표는 정보이론·비용 제약으로 불가능하다. 얼마나 정확히, 얼마나 싸게 볼 것인가가 과제다.

---

## 5.4 운영의 수학: SLO·리스크·비용의 다목적 최적화

AE의 의사결정은 본질적으로 다목적 최적화(Multi-Objective Optimization)다. 대표적 목적함수:

$$
\min_{x\in\mathcal{X}}
\;\;\alpha\cdot \text{Cost}(x)
+ \beta\cdot \text{Risk}(x)
+ \gamma\cdot \text{SLO\_Violation}(x)
$$

- Cost: 인스턴스/스토리지/네트워크 비용 (온디맨드 vs 스팟, 멀티AZ 등)
- Risk: 장애 확률, 스팟 중단 기대손실, 단일장애점(SPOF) 노출
- SLO Violation: p99 지연/오류율 페널티

제약:

$$
\text{p99Latency}(x)\le L^\*,\quad
\text{Avail}(x)\ge A^\*,\quad
\text{Budget}(x)\le B
$$

현실에서는 이 문제를 정확히 풀 수 없다. 대신 Pareto Optimal 해들을 탐색해 정책(Policy)로 굳힌다.

실제 사례 – Autoscaling/Spot 전략

- AWS EC2 Auto Scaling + Spot: 비용을 60\~90% 절감 가능하지만 중단 리스크가 증가.
- 보통은 혼합 포트폴리오(온디맨드 $+$ 스팟 $+$ 리저브드)를 구성해 CVaR(조건부 가치-at-리스크)를 낮춘다.
- GKE Autopilot도 유사하게 클래스별 SLO/비용 제약을 고려한 정책을 택한다.

---

## 5.5 요약

- 클라우드 배치는 멀티디멘션 Bin Packing/Knapsack (NP-hard) → 휴리스틱·온라인 최적화.
- 네트워크 라우팅은 단일 제약은 P, 다중 제약은 NP-hard → 관측 기반 동적 근사.
- Observability는 정보이론·스케치 구조가 한계를 규정 → 정확도–자원–비용의 명시적 트레이드오프.
- AE의 본질은 다목적 최적화의 정책화: 완전해 대신 Pareto 근사를 운영 전략으로 채택.

# 6장. P vs NP, 그리고 자연의 법칙

## 6.1 문제의 정의: P와 NP

컴퓨터 과학에서 가장 심오한 질문은 단순하다.
“모든 NP 문제는 다항시간 안에 풀 수 있는가?”

- P (Polynomial Time): 입력 크기 $n$에 대해, 다항시간 $O(n^k)$ 안에 해를 찾을 수 있는 문제.
- NP (Nondeterministic Polynomial Time): 해가 주어졌을 때, 다항시간 안에 그 해가 맞는지 검증할 수 있는 문제.

형식적으로는 다음 질문이다:

$$
P \stackrel{?}{=} NP
$$

---

## 6.2 P=NP라면?

만약 $P=NP$라면, 세상은 근본적으로 달라진다.

1. 암호학의 붕괴
   - RSA, ECC 등 공개키 암호는 소인수분해, 이산로그 문제가 다항시간에 풀 수 없다는 가정에 의존한다.
   - 만약 $P=NP$라면, 이 가정이 무너지고 인터넷 보안의 기반이 붕괴한다.

2. AI의 완전 자동화
   - 프로그램 자동 생성, 수학 증명 자동화, 단백질 접힘, 신약 개발… 모든 NP 문제를 다항시간 내 정밀하게 풀 수 있다.
   - 이는 사실상 “완전한 자동 프로그래머”의 출현과 다름없다.

3. 자연과학 난제 해결
   - Navier–Stokes 방정식의 해, 단백질 접힘, 물리·화학적 시뮬레이션의 정밀 계산이 현실화된다.
   - 인간이 수천 년간 싸운 문제들이 단숨에 tractable해진다.

즉, $P=NP$는 단순한 수학적 동치가 아니라, 문명 전체의 규칙이 다시 쓰이는 사건이다.

---

## 6.3 P≠NP라면?

대부분의 학자들은 $P \ne NP$에 무게를 둔다.
이는 곧 복잡도는 우리의 운명적 제약임을 뜻한다.

1. 근사 알고리즘의 필연성
   - 다항시간 근사 알고리즘(Approximation Algorithm)의 연구는 바로 $P \ne NP$를 전제로 한다.
   - 어떤 NP-hard 문제 $X$에 대해, 우리는 “해의 질”과 “계산 시간” 사이의 근사비율을 추구한다.

   예: Vertex Cover 문제 → 2-근사 알고리즘 보장

   $$
   \text{OPT} \le \text{ALG} \le 2 \cdot \text{OPT}
   $$

   즉, 완벽한 해 대신 “얼마나 가까운가”가 핵심 척도가 된다.

2. 평균적 난이도와 휴리스틱
   - 실무에서 우리가 푸는 문제는 최악의 경우(worst-case)가 아니라 평균적 경우(average-case)다.
   - 따라서 랜덤화 알고리즘, 휴리스틱, 메타휴리스틱(유전 알고리즘, 시뮬레이티드 어닐링 등)이 주류 전략이 된다.

3. 자연의 제약과 병행
   - 자연 또한 “완전한 해”가 아니라 “충분히 좋은 근사해”를 선택한다.
   - 진화 알고리즘 자체가 NP-hard한 공간에서 근사적 적합도를 찾는 과정으로 해석될 수 있다.

---

## 6.4 복잡도의 철학: 불가능을 운영하는 기술

따라서 P vs NP 문제는 단순한 이론적 질문이 아니라,
소프트웨어 엔지니어가 직면한 본질적 태도를 드러낸다.

- 프론트엔드: Virtual DOM, CSS 매칭 → NP-hard 문제의 근사.
- 백엔드: 쿼리 최적화, CAP 정리 → 불가능성 위의 절충.
- 데이터/AI: SGD, 옵티마이저 → NP-hard 최적화를 확률적으로 근사.
- 아키텍처: Bin Packing, 라우팅, Observability → NP-hard/정보이론적 제약 속 근사 운영.

결국 “엔지니어링이란 풀 수 없는 문제를 근사적으로 운영하는 기술”이다.

---

## 6.5 결론을 향해

- 만약 $P=NP$: 문명은 한순간에 새로운 질서로 전환된다.
- 그러나 $P\ne NP$: 우리는 불가능과 제약 속에서 근사, 휴리스틱, 타협, 운영을 지속해야 한다.

즉, 복잡도는 단순한 이론이 아니라,
자연과 코드가 공유하는 궁극적 법칙,
그리고 엔지니어의 운명적 제약이다.

# 7장. 결론 – 소프트웨어 엔지니어의 길

우리는 지금까지 복잡도의 그림자가 데이터, 프론트엔드, 백엔드, 아키텍처 모든 층위에 드리워져 있음을 살펴보았다.
결론은 단순하다.

“소프트웨어 엔지니어링이란 풀 수 없는 문제를 근사적으로 운영하는 기술”이다.

---

## 7.1 영역별 요약

| 영역           | 전형적 난제                                 | 복잡도 특성                               | 현실적 해법                                        |
| -------------- | ------------------------------------------- | ----------------------------------------- | -------------------------------------------------- |
| 데이터/AI      | Loss 최적화, SQL 조인 순서                  | NP-hard 최적화                            | SGD, Adam (확률적 근사), 비용 기반 옵티마이저      |
| 프론트엔드(FE) | CSS 선택자 매칭, DOM diffing                | NP-complete/Tree Edit Distance ($O(n^3)$) | Virtual DOM, Keyed diffing (휴리스틱)              |
| 백엔드(BE)     | Job Scheduling, 분산 트랜잭션, CAP          | NP-hard + 불가능성 정리                   | Priority Scheduling, DP+휴리스틱, CP/AP 절충       |
| 아키텍처(AE)   | Bin Packing, 멀티제약 라우팅, Observability | NP-hard / 정보이론적 한계                 | Heuristic 배치, Online Routing, 샘플링·스케치 구조 |

→ 공통된 패턴: 모두 “근사(approximation), 휴리스틱(heuristics), 트레이드오프(trade-offs)”를 통해 현실화된다.

---

## 7.2 트레이드오프의 도식

```
정확성 (Optimality)
   ▲
   │   불가능 (NP-hard, CAP, CR bound)
   │
   │        근사/휴리스틱 (Engineering Reality)
   │
   │
   └────────────────────────►
        시간/비용/자원 (Practicality)
```

- 왼쪽 위: 수학적 이상 (완전해, 최적화)
- 오른쪽 아래: 실제 시스템 (근사해, 운영 가능성)
- 엔지니어링은 이 축 위에서 Pareto 최적점을 탐색하는 과정이다.

---

## 7.3 근사의 미학

P≠NP일 가능성이 높다는 사실은 좌절이 아니라, 오히려 창의성의 원천이다.
풀 수 없는 문제를 다루는 과정에서 우리는 다음을 배운다:

- 완전해 대신 “쓸모 있는 해”를 찾는 지혜
- 휴리스틱 설계에서 오는 경험적 직관
- 제약 조건 속에서 새로운 알고리즘과 패턴을 발명하는 창의성

이것이 바로 근사의 미학이다.

---

## 7.4 엔지니어의 길

따라서 소프트웨어 엔지니어의 역할은 단순한 코드 작성이 아니다.
우리는 자연의 법칙과 수학적 제약이 허락한 범위 안에서,
가장 좋은 근사해를 가장 우아하게 운영하는 기술자다.

- 데이터 과학자는 NP-hard 최적화 대신 SGD를 선택한다.
- 프론트엔드 엔지니어는 NP-complete한 DOM diffing 대신 Virtual DOM을 선택한다.
- 백엔드 엔지니어는 CAP의 불가능성 속에서 CP 또는 AP를 선택한다.
- 아키텍처 엔지니어는 Bin Packing과 정보이론적 한계 속에서 휴리스틱과 샘플링을 선택한다.

즉, 우리가 쓰는 모든 기술은 “근사”라는 공통 언어 위에 세워져 있다.

---

# 맺음말

“복잡도는 운명인가?”라는 질문에 답하자면,
"그렇다!!"라고 단호하게 말씀 드릴 수 있습니다(사실은.. 사살이니).
그렇다고 이것을 인류의 제약 혹은 질 수 밖에 없는 게임이라고 치부하는 것은 안됩니다.
오히려 문제 해결을 위해 '근사'라는 창의적인 방법을 떠올리게 하는 영감의 근원으로 보면 어떨가요?

풀 수 없는 문제를 다루는 법을 익힌다는 것,
그 불가능성을 운영하는 법을 아는 것,
이것이 소프트웨어 엔지니어가 걸어야 하는 길 아닐까요?

마치겠습니다.

감사합니다!
